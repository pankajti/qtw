---
title: "case_study_week_6"
author: "Pankaj Kumar,  Brady Arendale , Kay Ayala"
date: "6/14/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(ggplot2)
require(reshape2)
library(rpart)
library(data.table)
library(rattle)
library(RColorBrewer)
library(data.tree)
library(h2o)
library(caret)
library(DiagrammeR)
library(tidyverse)

```

## R Markdown Plot email Exploring variables 



```{r cars}

getwd()
setwd("/Users/pankaj/dev/git/smu/qtw/case_study3/Week_5_Materials_2")
load("data.Rda") 

length(emailDFrp)


plot(colSums(is.na(emailDFrp)))

df1= emailDFrp

df1$id = seq(1, length(emailDFrp$isRe))


df1 %>% 
  ggplot(mapping = aes(x= isRe, fill = isSpam))+
  geom_bar()

df1 %>% 
  ggplot(mapping = aes(x= underscore, fill = isSpam))+
  geom_bar()

df1 %>% 
  ggplot(mapping = aes(x= priority, fill = isSpam))+
  geom_bar()


df1 %>% 
  ggplot(mapping = aes(x= isInReplyTo, fill = isSpam))+
  geom_bar()

df1 %>% 
  ggplot(mapping = aes(x= sortedRec, fill = isSpam))+
  geom_bar()

 
df1 %>% 
  ggplot(mapping = aes(x= subPunc, fill = isSpam))+
  geom_bar()

 
df1 %>% 
  ggplot(mapping = aes(x= multipartText, fill = isSpam))+
  geom_bar()

 
df1 %>% 
  ggplot(mapping = aes(x= hasImages, fill = isSpam))+
  geom_bar()

 
df1 %>% 
  ggplot(mapping = aes(x= isPGPsigned, fill = isSpam))+
  geom_bar()

 
df1 %>% 
  ggplot(mapping = aes(x= subSpamWords, fill = isSpam))+
  geom_bar()

 
df1 %>% 
  ggplot(mapping = aes(x= noHost, fill = isSpam))+
  geom_bar()

 
df1 %>% 
  ggplot(mapping = aes(x= numEnd, fill = isSpam))+
  geom_bar()

 
df1 %>% 
  ggplot(mapping = aes(x= isYelling, fill = isSpam))+
  geom_bar()

 
df1 %>% 
  ggplot(mapping = aes(x= isOrigMsg, fill = isSpam))+
  geom_bar()

 
df1 %>% 
  ggplot(mapping = aes(x= isDear, fill = isSpam))+
  geom_bar()

 
df1 %>% 
  ggplot(mapping = aes(x= isWrote, fill = isSpam))+
  geom_bar()

 
df1 %>% 
  ggplot(mapping = aes(x= subExcCt, fill = isSpam))+
  geom_bar()


df1 %>% 
  ggplot(mapping = aes(x= subQuesCt, fill = isSpam))+
  geom_bar()

 
df1 %>% 
  ggplot(mapping = aes(x= numLines, fill = isSpam))+
  geom_bar(binwidth = 100)


 
df1 %>% 
  ggplot(mapping = aes(x= numDlr, fill = isSpam))+
  geom_bar(binwidth = 100)

 
df1 %>% 
  ggplot(mapping = aes(x= numAtt, fill = isSpam))+
  geom_bar()

 
df1 %>% 
  ggplot(mapping = aes(x= numRec, fill = isSpam))+
  geom_bar(binwidth = 100)
 
df1 %>% 
  ggplot(mapping = aes(x= hour, fill = isSpam))+
  geom_bar()

 
df1 %>% 
  ggplot(mapping = aes(x= perHTML, fill = isSpam))+
  geom_bar()



df1 %>% 
  ggplot(mapping = aes(x= forwards ,    color = isSpam))+
  geom_freqpoly( binwidth= 1)


df1 %>% 
  ggplot(mapping = aes(x= perCaps ,    color = isSpam))+
  geom_freqpoly(binwidth= 1 )

df1 %>% 
  ggplot(mapping = aes(x= bodyCharCt ,    color = isSpam))+
  geom_freqpoly( binwidth= 100)

df1 %>% 
  ggplot(mapping = aes(x= avgWordLen ,    color = isSpam))+
  geom_freqpoly(binwidth= 1 )

df1 %>% 
  ggplot(mapping = aes(x= subBlanks ,    color = isSpam))+
  geom_freqpoly( binwidth= 1)

```


We cab see from above graphs there are some variables that dominate one type of messages more than others. For example variables isInReply to and  priority have just one value in Spam messages which means that they might be useful in splitting trees in tree based model.Other attributes like subPunk and multipartText are almost equally present in both types of messages. Similarly if 
we see isPGsigned True then the message are almost always non spam.


## Tree Model 


### Random Tree Model with 2 variables 


```{r pressure, echo=FALSE}
set.seed(2568)
n <- nrow(emailDFrp)
train <- sort(sample(1:n, floor(n/2)))

# Training data will be:

emailDFrp.train <- emailDFrp[train,]

# negate the indices to get the test data:
emailDFrp.test <- emailDFrp[-train,]


emailDFrp.rp1 <-rpart(isSpam ~ isRe+underscore ,                         # formula
                data = emailDFrp,                       # data frame
                subset = train,                       # indices of training set
                method = "class",                     # classification tree
                parms = list(split = "information"),  # use entropy/deviance
                maxsurrogate = 0,                     # since no missing values
                cp = 0,                               # no size penalty
                minsplit = 5,                         # Nodes of size 5 (or # more) can be split,
                minbucket = 2,                        # provided each sub-node
                # contains at least 2 obs.
)

pred.rpint1 <- predict(emailDFrp.rp1,
                       newdata = emailDFrp[-train,],
                       type = "class")




#summary(emailDFrp.rp1)

plot(emailDFrp.rp1, 
     uniform=TRUE,
     compress=TRUE,
     margin = .2)
text(emailDFrp.rp1, 
     use.n=TRUE, 
     all = TRUE,
     fancy = TRUE)

fancyRpartPlot(emailDFrp.rp1, main="Decision Tree Graph With 2 variables ")


#
#  Classification table on the test data
#

cm = table(emailDFrp$isSpam[-train], pred.rpint1)

confusionMatrix(cm)


```


### Grid search for tree model 

We can see that accuracy of above random model is not very high so we do a grid search on all variables and different tree hights to figure out which tree will be the best suited and where we ll get optmimal results without overfitting the model.


```{r }

train.control <- trainControl(
  method = "repeatedcv",
  number = 10, ## 10-fold CV
  repeats = 3,## repeated three times
  # USE AUC
  summaryFunction = twoClassSummary, 
  classProbs = TRUE
)

tune.gridcart <- expand.grid(maxdepth = 2:15)

#tune.gridcart <- expand.grid(cp = seq(2,10))


system.time (rpartFit2 <- train(isSpam ~ . ,                         
                                data = emailDFrp,
                                method = "rpart2", 
                                tuneGrid =tune.gridcart,
                                trControl = train.control,
                                metric = "ROC", na.action = na.pass
))



rpartFit2 

plot(rpartFit2)


```



From above grid search it looks like ROC curve area increases with number of depth but the increase is not much after max depth 6. so that one we ll choose for our final model 



#### Tree Model with all variables and maxdepth determined by grid search.


``` {r}


emailDFrp.rp <-rpart(isSpam ~ . ,                         # formula
                     data = emailDFrp,                       # data frame
                     subset = train,                       # indices of training set
                     method = "class",                     # classification tree
                     parms = list(split = "information"),  # use entropy/deviance
                     maxsurrogate = 0,                     # since no missing values
                     cp = 0,                               # no size penalty
                     minsplit = 5,                         # Nodes of size 5 (or # morecan be split,
                     minbucket = 2,
                     # provided each sub-node
                     # contains at least 2 obs.
                     control=rpart.control(maxdepth=6)
)


#summary(emailDFrp.rp)

plot(emailDFrp.rp, 
     uniform=TRUE,
     compress=TRUE,
     margin = .2)
text(emailDFrp.rp, 
     use.n=TRUE, 
     all = TRUE,
     fancy = TRUE)

fancyRpartPlot(emailDFrp.rp, main="Decision Tree Graph")


```
 

###  Plot and analysis of  the paths through  tree

As per the above we plot we can see first split is on variable forward. if the value is more than 5.5 another split is done on variable perCaps. We can see that 53% of total mails are in node with forward >6 and perCaps less than 6. This contains 73% of non spams and 27 % of smaps. We split this node further based on subblanks less than 24. 
Other nodes can be explained in same way and finally we reach leaf node.




 
#### Evaluation of Model 

confusion matrix for model based on all vars and max tree height 6.


```{r}

pred.rpint <- predict(emailDFrp.rp,
                       newdata = emailDFrp[-train,],
                       type = "class")


#
#  Classification table on the test data
#

cm = table(emailDFrp$isSpam[-train], pred.rpint)

print(confusionMatrix(cm))

```

As we can see the accurace of this model is 87% on test data.


### Variables that are most important for the model we found with grid search 

Most important variables determined by tree model are as per following attribute of model.


```{r}

emailDFrp.rp$variable.importance

```


We cab see the forwards , isInReplyTo and isRe etc. are among the most important variables. We found during our exploratory data analysis the similar conclusion. One type of emails were dominated by some specific variables more than other was. 




#### H2O function definitions 



```{r}
# split into train and validation


library(data.tree)

createDataTree <- function(h2oTree) {
  h2oTreeRoot = h2oTree@root_node
  dataTree = Node$new(h2oTreeRoot@split_feature)
  dataTree$type = 'split'
  addChildren(dataTree, h2oTreeRoot)
  return(dataTree)
}

addChildren <- function(dtree, node) {
  
  if(class(node)[1] != 'H2OSplitNode') return(TRUE)
  
  feature = node@split_feature
  id = node@id
  na_direction = node@na_direction
  
  if(is.na(node@threshold)) {
    leftEdgeLabel = printValues(node@left_levels, 
                                na_direction=='LEFT', 4)
    rightEdgeLabel = printValues(node@right_levels, 
                                 na_direction=='RIGHT', 4)
  }else {
    leftEdgeLabel = paste("<", node@threshold, 
                          ifelse(na_direction=='LEFT',',NA',''))
    rightEdgeLabel = paste(">=", node@threshold, 
                           ifelse(na_direction=='RIGHT',',NA',''))
  }
  
  left_node = node@left_child
  right_node = node@right_child
  
  if(class(left_node)[[1]] == 'H2OLeafNode')
    leftLabel = paste("prediction:", left_node@prediction)
  else
    leftLabel = left_node@split_feature
  
  if(class(right_node)[[1]] == 'H2OLeafNode')
    rightLabel = paste("prediction:", right_node@prediction)
  else
    rightLabel = right_node@split_feature
  
  if(leftLabel == rightLabel) {
    leftLabel = paste(leftLabel, "(L)")
    rightLabel = paste(rightLabel, "(R)")
  }
  
  dtreeLeft = dtree$AddChild(leftLabel)
  dtreeLeft$edgeLabel = leftEdgeLabel
  dtreeLeft$type = ifelse(class(left_node)[1] == 'H2OSplitNode', 'split', 'leaf')
  
  dtreeRight = dtree$AddChild(rightLabel)
  dtreeRight$edgeLabel = rightEdgeLabel
  dtreeRight$type = ifelse(class(right_node)[1] == 'H2OSplitNode', 'split', 'leaf')
  
  addChildren(dtreeLeft, left_node)
  addChildren(dtreeRight, right_node)
  
  return(FALSE)
}

printValues <- function(values, is_na_direction, n=4) {
  l = length(values)
  if(l == 0)
    value_string = ifelse(is_na_direction, "NA", "")
  else
    value_string = paste0(paste0(values[1:min(n,l)], collapse = ', '),
                          ifelse(l > n, ",...", ""),
                          ifelse(is_na_direction, ", NA", ""))
  return(value_string)
}

```


### H2O random forest model 

We use random forest from h2o lib to generate other tree.

```{r h2oplot , fig.height = 10, fig.width = 10, fig.align = "center"}

#knitr::opts_chunk$set(fig.width=12, fig.height=8) 

# Build and train the model:

h2o.init()

emailDHex = as.h2o(emailDFrp)

splits = h2o.splitFrame(data = emailDHex, ratios = .8, seed = 1234)
trainHex = splits[[1]]
validHex = splits[[2]]
predictors = c("isRe")
predictors = names(emailDFrp)[c(seq(2,29))]
response = c("isSpam")


emailD_2tree <- h2o.randomForest(x = predictors,
                                  y = response,
                                  ntrees = 5,
                                  max_depth = 5,
                                  min_rows = 20,
                                  binomial_double_trees = TRUE,
                                  training_frame = trainHex,
                                  validation_frame = validHex)

emailDH2oTree2 = h2o.getModelTree(model = emailD_2tree, tree_number = 2)

library(data.tree)


library(DiagrammeR)

emailDataTree = createDataTree(emailDH2oTree2)

GetEdgeLabel <- function(node) {return (node$edgeLabel)}
GetNodeShape <- function(node) {switch(node$type, 
                                       split = "diamond", leaf = "oval")}
GetFontName <- function(node) {switch(node$type, 
                                      split = 'Palatino-bold', 
                                      leaf = 'Palatino')}
SetEdgeStyle(emailDataTree, fontname = 'Palatino-italic', 
             label = GetEdgeLabel, labelfloat = TRUE,
             fontsize = "26", fontcolor='royalblue4')
SetNodeStyle(emailDataTree, fontname = GetFontName, shape = GetNodeShape, 
             fontsize = "26", fontcolor='royalblue4',
             height="1.75", width="2")

SetGraphStyle(emailDataTree, rankdir = "LR", dpi=70.)
dev.new(width=10, height=10)

plot(emailDataTree, output = "graph")

emailDataTree

h2o.shutdown()


```



We found almost similar variables in slightly different order as we found in our initial tree model.


