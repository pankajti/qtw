{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 6\n",
    "\n",
    "<font size=4>Brady Arendale, Pankaj Kumar, Kay Ayala  \n",
    "7/29/20</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We were asked to replicate a neural network architecture from the paper [\"Searching for Exotic Particles in High-Energy Physics with Deep Learning\"](https://arxiv.org/pdf/1402.4735.pdf). We will create the architecture in TensorFlow as originally formulated, and then suggest improvements that could be made based on developments in deep learning since the paper was published in 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow import random_normal_initializer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "learning_rate = 0.05\n",
    "opt = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "init_first = random_normal_initializer(stddev=0.1)\n",
    "init_hidden = random_normal_initializer(stddev=0.05)\n",
    "init_output = random_normal_initializer(stddev=0.001)\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=10)\n",
    "\n",
    "model.add(Dense(300, activation='tanh', kernel_initializer=init_first))\n",
    "model.add(Dense(300, activation='tanh', kernel_initializer=init_hidden))\n",
    "model.add(Dense(300, activation='tanh', kernel_initializer=init_hidden))\n",
    "model.add(Dense(300, activation='tanh', kernel_initializer=init_hidden))\n",
    "model.add(Dense(300, activation='tanh', kernel_initializer=init_hidden))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer=init_output))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['auc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors of the paper used a neural network with 5 hidden layers of 300 units each. The final layer outputs a classification. Each hidden layer used a tanh activation function. The activation function for the last layer was not specified, so we used the standard sigmoid activation. The first hidden layer was initialized with normally distributed weights with a mean of 0 and a standard deviation of 0.1. The other hidden layers used a mean of 0 and a standard deviation of 0.05, and the output layer used a mean of 0 and a standard deviation of 0.001.\n",
    "\n",
    "The authors used stochastic gradient descent to optimize the neural network. They used an intial learning rate of 0.05 with a decay defined below. They used a momentum of 0.9 that increased linearly to 0.99 over 200 epochs. We did not find any way to change the momentum mid-training in TensorFlow, so we locked the momentum at 0.9. They did not specify a loss function, so we used the standard cross-entropy loss function. AUC was used as an evaluation metric. They use early stopping to prevent overfitting. The early stopping criterion was no decrease in validation loss by more than a factor of 0.00001 over 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateCallback(tensorflow.keras.callbacks.Callback):\n",
    "  \n",
    "    def __init__(self, lr_decay_factor=1.0000002, update_freq=None, min_lr=1e-6):\n",
    "        self._update_freq = update_freq\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_decay_factor = lr_decay_factor\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self._update_freq and batch % self._update_freq != 0:\n",
    "            return\n",
    "        \n",
    "        if self.learning_rate <= self.min_lr:\n",
    "            self.learning_rate = self.min_lr\n",
    "        else:    \n",
    "            self.learning_rate = self.learning_rate/self.lr_decay_factor\n",
    "        tensorflow.keras.backend.set_value(self.model.optimizer.lr, self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate decays by a factor of 1.0000002 per batch, and stops decaying upon reaching the minimum learning rate of 10^-6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_callback = LearningRateCallback()\n",
    "\n",
    "# model.fit(X_train, y_train, epochs=1000, batch_size=100, callbacks=[es, learning_rate_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our model fitting function. We use a batch size of 100 like the authors. We add in our early stopping and learning rate decay callbacks. We train for 1000 epochs, which is the maximum number of epochs the authors said their networks trained for. However, our early stopping criterion is likely to be reached before that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "Since deep learning has evolved much over the last 6 years, there are some recommendations we can make based on research since the original paper and more powerful processors. We can experiment with more layers, more number of neurons per layers and other combination of activation functions. We can also try different batch sizes. We expect to find better approximation of analytical functions by increasing complexity. We can experiment with learning rate to address the problem of vanishing gradient descent. We can take partial features from low-level and high-level features. For now the 3 set of features that are used are low-level, high-level and complete. We can exclude some low- and high-level features and train with deeper models to get better results. Experiment with different or custom loss functions with features like label smoothing.  We expect to converge to global minima at faster rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "In addition to the above, there are several practices that have become standard since the paper was written. First is the use of different activation functions, particularly ReLU. Another is the creation of more advanced optimizers like Adam. Adam automatically adjusts the learning rate adaptively for each paramter, rather than using complicated arbitrary learning rate and momentum updates like in the paper. The combination of ReLU and Adam has been very powerful in improving convergence and combating vanishing and exploding gradient descent.\n",
    "\n",
    "Although the authors experimented with dropout, there are now many regularization techniques in common use. These include L1 and L2 regularization and batch normalization. These techniques have been shown to improve generalization in many cases, and may improve the performance of the model.\n",
    "\n",
    "One last standard practice nowadays is the use initialization techniques such as Glorot or Kaiming intialization. These have also been shown to improve convergence compared to sampling from arbitrary uniform or normal distributions. These techniques use input and/or output shapes to determine either the range of a uniform distribution or the standard deviation of a normal distribution to sample from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "We can have a good idea of whether we successfully replicated the authors' paper by comparing AUC scores. The authors reported an AUC score of 0.885 on their best-performing model. We would expect to see something similar. However, we will not replicate the exact model due to differences in things like random intialization and random batch shuffling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We replicated the authors' paper as accurately as possible within in the TensorFlow/Keras framework. We suggested possible improvements that could be made such as different number of layers and neurons and different feature selection. We mentioned modern standard practices like ReLU and Adam that could also be used to improve the model. With these improvements, we may be able to achieve a higher AUC and contribute to the success of collider searches for exotic particles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
